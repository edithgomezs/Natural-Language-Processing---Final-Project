{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8kHct9-VhAf1"
      },
      "outputs": [],
      "source": [
        "!pip install -q kaggle\n",
        "from google.colab import files\n",
        "import json\n",
        "import pandas as pd\n",
        "import gc\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import HTML, display\n",
        "import time\n",
        "!pip install transformers\n",
        "from transformers import Trainer\n",
        "from transformers import TrainingArguments\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, DataCollatorWithPadding\n",
        "!pip install datasets\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "import re\n",
        "import tensorflow as tf\n",
        "from numpy import mean\n",
        "from numpy import std\n",
        "from sklearn.datasets import make_regression\n",
        "from sklearn.model_selection import RepeatedKFold\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "# Serializing json\n",
        "# Enter a username and API key in these strings to automatically download Kaggle data\n",
        "json_object = json.dumps({\"username\":\"_\",\"key\":\"_\"}, indent=4)\n",
        " \n",
        "# Writing to kaggle.json\n",
        "with open(\"kaggle.json\", \"w\") as outfile:\n",
        "    outfile.write(json_object)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3IhFZJOhX6Y"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json\n",
        "! kaggle datasets download -d yelp-dataset/yelp-dataset\n",
        "! mkdir train\n",
        "! unzip -o yelp-dataset.zip -d train\n",
        "! rm yelp-dataset.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZLc7RWoZuEIb"
      },
      "outputs": [],
      "source": [
        "max = 0\n",
        "def progress(value):\n",
        "    return HTML(\"\"\"\n",
        "        <progress\n",
        "            value='{value}'\n",
        "            max='{max}',\n",
        "            style='width: 90%'\n",
        "        >\n",
        "            {value}\n",
        "        </progress>\n",
        "    \"\"\".format(value=value, max=max))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U1vqW1-Rijc_"
      },
      "outputs": [],
      "source": [
        "# Read in and process data\n",
        "max = 6290515\n",
        "chunk = 100000\n",
        "chunks = pd.read_json('train/yelp_academic_dataset_review.json', lines=True, chunksize=chunk)\n",
        "df = pd.DataFrame()\n",
        "\n",
        "out = display(progress(0), display_id=True)\n",
        "i = chunk\n",
        "\n",
        "for c in chunks:\n",
        "    df = pd.concat([df, c])\n",
        "    out.update(progress(i))\n",
        "    i += chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ-ZpYTM1MDD"
      },
      "outputs": [],
      "source": [
        "print(df[\"funny\"].max())\n",
        "print(df[\"funny\"].mean())\n",
        "print(df[\"useful\"].max())\n",
        "print(df[\"useful\"].mean())\n",
        "print(df[\"cool\"].max())\n",
        "print(df[\"cool\"].mean())\n",
        "\n",
        "def setHistLabels(title):\n",
        "  plt.title(title + ' Score Distribution')\n",
        "  plt.xlabel('Number of Votes')\n",
        "  plt.ylabel('Number of Reviews')\n",
        "\n",
        "hist = df.hist(bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"funny\"])\n",
        "setHistLabels(\"Funny\")\n",
        "hist1 = df.hist(bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"useful\"])\n",
        "setHistLabels(\"Useful\")\n",
        "hist2 = df.hist(bins=[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"cool\"])\n",
        "setHistLabels(\"Cool\")\n",
        "hist = df.hist(bins=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"funny\"])\n",
        "setHistLabels(\"Funny without 0\")\n",
        "hist1 = df.hist(bins=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"useful\"])\n",
        "setHistLabels(\"Useful without 0\")\n",
        "hist2 = df.hist(bins=[1, 2, 3, 4, 5, 6, 7, 8, 9, 10], column=[\"cool\"])\n",
        "setHistLabels(\"Cool without 0\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4IYlm4Lm62He"
      },
      "outputs": [],
      "source": [
        "def findCorrelations(column, colTitle):\n",
        "  print(\"Correlation between \" + colTitle + \" and Funny scores\", column.corr(df['funny']))\n",
        "  print(\"Correlation between \" + colTitle + \" and Useful scores\", column.corr(df['useful']))\n",
        "  print(\"Correlation between \" + colTitle + \" and Cool scores\", column.corr(df['cool']), \"\\n\")\n",
        "\n",
        "def getAverageWordLength(n):\n",
        "  words = n.split()\n",
        "  return sum(len(word) for word in words) / len(words)\n",
        "\n",
        "findCorrelations(df[\"stars\"], \"Star Rating\")\n",
        "findCorrelations(df[\"text\"].apply(lambda n: len(n.split())), \"Word Count\")\n",
        "findCorrelations(df[\"text\"].apply(getAverageWordLength), \"Average Word Length\")\n",
        "df.corr()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AcGxn9FzfYqu"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(16, 6))\n",
        "heatmap = sns.heatmap(df.corr(), vmin=-1, vmax=1, annot=True)\n",
        "# Give a title to the heatmap. Pad defines the distance of the title from the top of the heatmap.\n",
        "heatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huNxvbN1RQW9"
      },
      "outputs": [],
      "source": [
        "def getCategoryResults(df, useful, funny, cool):\n",
        "    mask = ((df['useful'] > 1 if useful else df['useful'] == 0)\n",
        "            & (df['funny'] > 1 if funny else df['funny'] == 0)\n",
        "            & (df['cool'] > 1 if cool else df['cool'] == 0 ))\n",
        "    return df.loc[mask,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BjM8E-V3GO2i"
      },
      "outputs": [],
      "source": [
        "# Need a dataframe of a sample size with text, star rating, user stats\n",
        "df.drop(columns=['business_id', 'date'], inplace=True)\n",
        "gc.collect()\n",
        "\n",
        "data = pd.DataFrame()\n",
        "\n",
        "# Min number of rows out of these categories\n",
        "dividedNum = 1000\n",
        "data = pd.concat([data, getCategoryResults(df, True, False, False).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, False, True, False).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, False, False, True).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, True, True, False).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, True, False, True).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, False, True, True).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, True, True, True).sample(dividedNum)])\n",
        "data = pd.concat([data, getCategoryResults(df, False, False, False).sample(dividedNum)])\n",
        "\n",
        "data = data.sample(frac=1)\n",
        "del df\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uXL9AeXJrEJg"
      },
      "outputs": [],
      "source": [
        "chunk = 100000\n",
        "max = 1987896\n",
        "chunks = pd.read_json('train/yelp_academic_dataset_user.json', lines=True, chunksize=chunk)\n",
        "users = pd.DataFrame()\n",
        "\n",
        "out = display(progress(0), display_id=True)\n",
        "i = chunk\n",
        "\n",
        "for c in chunks:\n",
        "    users = pd.concat([users, c])\n",
        "    out.update(progress(i))\n",
        "    i += chunk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGzxVBeDrWAJ"
      },
      "outputs": [],
      "source": [
        "# Renaming columns to prevent clash between useful, funny, cool columns in the review and user datasets\n",
        "data.rename(columns={\"useful\": \"review_useful\", \"funny\": \"review_funny\", \"cool\": \"review_cool\"}, inplace=True)\n",
        "data = pd.merge(data, users[[\"user_id\", \"useful\", \"funny\", \"cool\"]], on=[\"user_id\"])\n",
        "userScoreSums = data[[\"useful\", \"funny\", \"cool\"]].sum(axis=1)\n",
        "data[\"user_useful\"] = (data[\"useful\"] / userScoreSums).fillna(1/3)\n",
        "data[\"user_funny\"] = (data[\"funny\"] / userScoreSums).fillna(1/3)\n",
        "data[\"user_cool\"] = (data[\"cool\"] / userScoreSums).fillna(1/3)\n",
        "data.drop(columns=['useful', 'funny', 'cool'], inplace=True)\n",
        "data.rename(columns={\"review_useful\": \"useful\", \"review_funny\": \"funny\", \"review_cool\": \"cool\"}, inplace=True)\n",
        "data.drop(columns=['user_id'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QrHJXIctXJj_"
      },
      "outputs": [],
      "source": [
        "del users\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NloarP5RGNcP"
      },
      "outputs": [],
      "source": [
        "X_train, X_test, _, _ = train_test_split(data, data[\"useful\"], train_size=0.8)\n",
        "\n",
        "del data\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jAXpsK89S-aJ"
      },
      "outputs": [],
      "source": [
        "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "!unzip -q glove.6B.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PaSP5kjTJrs"
      },
      "outputs": [],
      "source": [
        "embeddings_index = {}\n",
        "with open(\"glove.6B.100d.txt\") as f:\n",
        "    for line in f:\n",
        "        word, coefs = line.split(maxsplit=1)\n",
        "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "print(\"Found %s word vectors.\" % len(embeddings_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2UYxr783Tc60"
      },
      "outputs": [],
      "source": [
        "def getWordEmbedding(text):\n",
        "  num_tokens = 256\n",
        "  embedding_dim = 100\n",
        "\n",
        "  # Prepare embedding matrix\n",
        "  embedding_matrix = np.zeros((num_tokens, embedding_dim))\n",
        "  for i, word in enumerate(text):\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        # Words not found in embedding index will be all-zeros.\n",
        "        # This includes the representation for \"padding\" and \"OOV\"\n",
        "        embedding_matrix[i] = embedding_vector\n",
        "  return embedding_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ge9sObYwWSE8"
      },
      "outputs": [],
      "source": [
        "def processX(x):\n",
        "  x[\"text\"] = x[\"text\"].apply(lambda text: re.sub('[^A-Za-z0-9]+', ' ', text.lower()).split()[0:256])\n",
        "  x[\"text\"] = x[\"text\"].apply(getWordEmbedding)\n",
        "  x['text'] = x['text'].apply(lambda text: np.concatenate(text).ravel()).values\n",
        "  X = x[[\"text\", \"stars\", \"user_useful\", \"user_funny\", \"user_cool\"]].values\n",
        "  y = x[[\"useful\", \"funny\", \"cool\"]].values\n",
        "  del x\n",
        "  gc.collect()\n",
        "  X = np.apply_along_axis(lambda row: np.concatenate([row[0].tolist(), row[1:4]]).ravel(), 1, X)\n",
        "  X = np.asarray(X).astype(np.float32)\n",
        "  return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4W3OMDxhAS30"
      },
      "outputs": [],
      "source": [
        "x_train, y_train = processX(X_train)\n",
        "del X_train\n",
        "gc.collect()\n",
        "x_test, y_test = processX(X_test)\n",
        "del X_test\n",
        "gc.collect()\n",
        "del embeddings_index\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hWmt80GELS8S"
      },
      "outputs": [],
      "source": [
        "# https://machinelearningmastery.com/deep-learning-models-for-multi-output-regression/\n",
        "# mlp for multi-output regression\n",
        "\n",
        "# get the model\n",
        "def get_model(n_inputs, n_outputs):\n",
        "\tmodel = Sequential()\n",
        "\tmodel.add(Dense(20, input_dim=n_inputs, \\\n",
        "\t\tkernel_initializer='he_uniform', activation='relu'))\n",
        "\tmodel.add(Dense(20, activation='relu'))\n",
        "\tmodel.add(Dense(20, activation='relu'))\n",
        "\tmodel.add(Dense(20, activation='relu'))\n",
        "\tmodel.add(Dense(20, activation='relu'))\n",
        "\tmodel.add(Dense(20, activation='relu'))\n",
        "\tmodel.add(Dense(n_outputs))\n",
        "\tmodel.compile(loss='mae', optimizer='adam')\n",
        "\treturn model\n",
        "\n",
        "# evaluate a model using repeated k-fold cross-validation\n",
        "def evaluate_model(ratingIndex):\n",
        "\tresults = list()\n",
        "\tn_inputs, n_outputs = x_train.shape[1], len(y_train[:, ratingIndex])\n",
        "\t# define model\n",
        "\tmodel = get_model(n_inputs, n_outputs)\n",
        "\t# fit model\n",
        "\tmodel.fit(x_train, y_train[:, ratingIndex], verbose=1, epochs=100)\n",
        "\t# evaluate model on test set\n",
        "\tmae = model.evaluate(x_test, y_test[:, ratingIndex], verbose=1)\n",
        "\t# store result\n",
        "\tprint('>%.3f' % mae)\n",
        "\tresults.append(mae)\n",
        "\treturn results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "546EOr8nVG__"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "results = evaluate_model(0)\n",
        "# summarize performance\n",
        "print('Useful - MAE: %.3f' % (mean(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWIKAGJiVIPh"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "results = evaluate_model(1)\n",
        "# summarize performance\n",
        "print('Funny - MAE: %.3f (%.3f)' % (mean(results), std(results)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1WZKMLJDVKV1"
      },
      "outputs": [],
      "source": [
        "# evaluate model\n",
        "results = evaluate_model(2)\n",
        "# summarize performance\n",
        "print('Cool - MAE: %.3f (%.3f)' % (mean(results), std(results)))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}